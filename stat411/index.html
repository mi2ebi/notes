<!doctype html>
<html>
  <head>
    <title>stat 411</title>
    <link rel="stylesheet" href="https://mi2ebi.github.io/style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
     dt.procedure {
       color: var(--green-warmer);
     }
     .example {
       color: var(--magenta-warmer);
     }
    </style>
  </head>
  <body>
    <h1 class="h0">stat 411</h1>
    <h1>ch1 basics</h1>
    <h2>1.1 intro/overview</h2>
    <dl>
      <dt>statistics</dt>
      <dd>methods to collect/analyse/present/interpret data <i>inductively</i></dd>
      <dd>science of data, decisionmaking, methodology</dd>
    </dl>
    <p>types of statistics:</p>
    <ol>
      <li>
        <b>descriptive</b> &ndash;
        methods to organise/display/decribe data
      </li>
      <li>
        <b>inferential</b> &ndash;
        methods to utitlse data to make decisions and models etc
      </li>
    </ol>
    <p>the generalisation from sample to population is <b>statistical inference</b></p>
    <h3>history</h3>
    <p>aristotle combined empricism and rationalism</p>
    <p>see also <i lang="la">cōgitō ergō sum</i></p>
    <p>
      after the industrial revolution there was more of an economy, and governments
      needed to find out different information about their populations
    </p>
    <blockquote>
      there are 3 lies in society: lies, damned lies, and statistics<br />
      <cite>&ndash; popularised by mark twain</cite>
    </blockquote>
    <p>book rec: <i>how to lie with statistics</i></p>
    <h2>1.3 experiments and data</h2>
    <p>no two measurements are ever exactly the same</p>
    <h2>1.4 inferences</h2>
    <dl>
      <dt>population</dt>
      <dd>
        all elements under study being characterized by some <b>parameter</b>:
        eg population mean, \(\mu\)
      </dd>
      <dt>sample</dt>
      <dd>
        a properly chosen subset of a population characterized by a <b>statistic</b>:
        eg sample mean, \(\bar x\)
      </dd>
      <dt>sample space</dt>
      <dd>collection of all possible outcomes of an experiment</dd>
      <dt>random sample</dt>
      <dd>
        a sample where every element of the population has the same likelihood to be
        chosen (assuming the population is finite)
      </dd>
      <dt class="procedure">simple random sample</dt>
      <dd class="dedent"><ol>
        <li>define the population</li>
        <li>pick a sample size \(n\)</li>
        <li>randomly select a sample eg using an rng</li>
      </ol></dd>
    </dl>
    <p><img src="sampling.png" /></p>
    <h2>1.5 data</h2>
    <dl>
      <dt>element/object/subject</dt>
      <dd>something we collect data about</dd>
      <dt>variable</dt>
      <dd>a characteristic being studied</dd>
      <dt>observation/measurement</dt>
      <dd>numerical values of a variable for an element</dd>
    </dl>
    <h3>places to get data from</h3>
    <p>census, survey, experiment, simulation, ...</p>
    <p>but to get scientific data we need 2 <b>statistical studies</b></p>
    <dl>
      <dt>observational study</dt>
      <dd>conducted in a way where we have no control over any variables</dd>
      <dd>
        <b>cross-sectional</b> &ndash;
        collect data at one point in time
      </dd>
      <dd>
        <b>longitudinal/cohort</b> &ndash;
        collect data over a period of time about common factors
      </dd>
      <dt>experimental study</dt>
      <dd>
        we have control over 1 variable. <b>treatment</b> group and <b>control</b>
        group. eg clinical trials
      </dd>
      <dd>
        principles (cf ch13-15 which we're not covering):<br />
        <b>replication</b>, <b>randomization</b>, <b>blocking/blinding</b>
      </dd>
    </dl>
    <h2>1.6 types of variables</h2>
    <dl>
      <dt>quantitative variables</dt>
      <dd><b>discrete</b> &ndash; countable</dd>
      <dd><b>continuous</b> &ndash; any numerical value</dd>
      <dt>qualitative/categorical variables</dt>
      <dd><b>nominal</b> &ndash; not rankable</dd>
      <dd><b>ordinal</b> &ndash; rankable</dd>
    </dl>
    <p>levels of measurement</p>
    <ul>
      <li>nominal</li>
      <li>ordinal</li>
      <li>
        <b>interval</b> &ndash;
        no such thing as absolute zero. eg we can't say las vegas is twice as hot as
        chicago if it's 60&deg; in vegas and 30&deg; in chicago
      </li>
      <li><b>ratio</b> &ndash; there is a possible absolute zero</li>
    </ul>
    <h2>1.3 measures of central tendency</h2>
    <p>if we have a random sample \(x_1, x_2, \dots, x_n\) of size \(n\)</p>
    <dl>
      <dt>measure of central tendency</dt>
      <dd>some indication of where the center of the dataset is</dd>
      <dt>mean</dt>
      <dd>simplest and nicest. "center of gravity"</dd>
      <dd>\(
        \=x = \frac1n \sum\limits_{i = 1}^n x_i = \sum\limits_{i = 1}^n \frac{x_i}n
      \)</dd>
      <dt>trimmed mean</dt>
      <dd>
        mean with some of the potential outliers cut out, eg 5% trimmed mean means
        taking only the middle 90% of the data
      </dd>
      <dt>median</dt>
      <dd>middle value in the ordered dataset</dd>
      <dd>if \(n\) is odd, \(\~x = x_{\frac{n + 1}2}\)</dd>
      <dd>
        if \(n\) is even, \(\~x = \frac12\left(x_{\frac n2} + x_{\frac n2 + 1}\right)\)
      </dd>
      <dd>median is less sensitive to outliers so it is more <b>robust</b></dd>
      <dt>mode</dt>
      <dd>values that occur most frequently</dd>
      <dd>one mode &rarr; <b>unimodal</b>, two modes &rarr; <b>bimodal</b>, etc</dd>
    </dl>
    <p>if the histogram of the data is symmetric, \(\text{mode} = \~x = \=x\)</p>
    <p>if the histogram is skewed to (has a longer tail on) the right, \(
      \text{mode} &lt; \~x &lt; \=x
    \)</p>
    <h2>1.4 measures of variability</h2>
    <dl>
      <dt>range</dt>
      <dd>difference in the max and min of the dataset</dd>
      <dt>(individual) deviation</dt>
      <dd>\(x_i - \=x\)</dd>
      <dt>sample variance</dt>
      <dd>\(
        s^2
        = \frac1{n - 1} \sum\limits_{i = 1}^n (x_i - \=x)^2
        = \frac1{n - 1} \left(
          \sum\limits_{i = 1}^n x_i^2 - \frac{\left(\sum x_i\right)^2}n
        \right)
      \)</dd>
      <dt>standard deviation</dt>
      <dd>\(s = \sqrt{s^2}\)</dd>
    </dl>
    <p>the second form is nicer for computation since you don't need eg 2 loops</p>
<pre>
<span class="keyword">let mut</span> <span class="variable">sum_squares</span> @ <span class="keyword">mut</span> <span class="variable">sum</span> = <span class="constant">0</span>;
<span class="keyword">let</span> <span class="variable">n</span> = <span class="variable">sample</span>.<span class="function">len</span>() <span class="keyword">as</span> <span class="type">f64</span>;
<span class="keyword">for</span> <span class="variable">x</span> <span class="keyword">in</span> <span class="variable">sample</span> {
  <span class="variable">sum</span> += <span class="variable">x</span>;
  <span class="variable">sum_squares</span> += <span class="variable">x</span> * <span class="variable">x</span>;
}
<span class="keyword">let</span> <span class="variable">s2</span> = <span class="constant">1.</span>/(<span class="variable">n</span>-<span class="constant">1.</span>) * (<span class="variable">sum_squares</span> - <span class="variable">sum</span>*<span class="variable">sum</span>/<span class="variable">n</span>);
<span class="keyword">let</span> <span class="variable">s</span> = <span class="variable">s2</span>.<span class="function">sqrt</span>();
</pre>
    <p>the sum of deviations is \(\sum\limits_{i = 1}^n (x_i - \=x) = 0\)</p>
    <dl>
      <dt>coefficient of variation</dt>
      <dd>for samples: \(\text{CV} = 100\% \cdot s/\=x\)</dd>
      <dd>for populations: \(\text{CV} = 100\% \cdot \sigma/\mu\)</dd>
      <dt>quartiles</dt>
      <dd>divide the sample into 4 equal parts (by #observations)</dd>
      <dd>\(Q_1\), med, \(Q_3\)</dd>
      <dt>interquartile range</dt>
      <dd>\(\text{IQR} = Q_3 - Q_1\)</dd>
      <dd>middle 50% of the sample</dd>
      <dt>five-number summary</dt>
      <dd>the three quartiles along with min and max</dd>
      <dd>used to make a <b>boxplot</b></dd>
      <dt>inner fence</dt>
      <dd>the range between \(Q_1 - 1.5~\text{IQR}\) and \(Q_3 + 1.5~\text{IQR}\)</dd>
      <dd>outside this are <b>mild outliers</b></dd>
      <dt>percentiles</dt>
      <dd>like quartiles but with 100 equal parts</dd>
      <dd>\(Q_1 = p_{25}\) etc</dd>
      <dd>
        \(p_k = \frac{nk}{100}\)th observation and says that \(k\)% of the observations
        are at or below that value
      </dd>
      <dt>percentile rank</dt>
      <dd>figure out what percentile an observation is</dd>
      <dd>\(100 \cdot (\text{\# observations} &lt; x_i) / n\)</dd>
    </dl>
    <h2>1.10 empirical rule & z-score</h2>
    <dl>
      <dt>empirical rule</dt>
      <dd>
        for a normal (bell shaped) distribution, <ul>
          <li>\(\=x \pm s\) contains 68% of values</li>
          <li>\(\=x \pm 2s\) &rarr; 95%</li>
          <li>\(\=x \pm 3s\) &rarr; 99.7%</li>
        </ul>
        so almost the entire dataset fits in 6 standard deviations.
        this is only a rule of thumb. also called the <b>6&sigma; rule</b>
      </dd>
      <dt>z-score</dt>
      <dd>\(z_i = \frac{x_i - \=x}s\)</dd>
      <dd>unit-free!</dd>
      <dd>if \(|z| &gt; 2\) for an observation it could be considered an outlier</dd>
      <dd>converting \(x_i\) to \(z_i\) is called <b>standardization</b></dd>
      <dd>and \(x_i = \=x + z_is\) for <b>destandardization</b></dd>
    </dl>
    <h1>ch2 probability</h1>
    <h2>2.1 intro</h2>
    <dl>
      <dt>statistical experiment</dt>
      <dd>process of generating outcomes that cannot be predicted in advance</dd>
      <dt>sample space</dt>
      <dd>set of all possible outcomes. denoted \(S\) or \(\Omega\)</dd>
      <dt>event</dt>
      <dd>any subset of \(S\), denoted with a capital letter</dd>
    </dl>
    <p>eg rolling a die: \(S = \set{1, 2, 3, 4, 5, 6}\)</p>
    <dl>
      <dt>probability</dt>
      <dd>numerical measure of how likely some event is, denoted \(P(\cdot)\)</dd>
    </dl>
    <p>kolmogorov's axioms of probability:</p>
    <ol>
      <li>\(0 \le P(A) \le 1\) for any event \(A\)</li>
      <li>\(P(S) = 1\)</li>
    </ol>
    <p>
      assuming all outcomes in \(S\) are equally likely,
      \(P(A) = \frac{\#A}{\#S}\)
    </p>
    <dl>
      <dt>venn diagram</dt>
      <dd>depiction of the relationship btwn sample space and events</dd>
      <dt>tree diagram</dt>
      <dd>(pp37-38)</dd>
    </dl>
    <h2>2.2 set operations</h2>
    <dl>
      <dt>empty set</dt>
      <dd>\(\emptyset\), the set with nothing in it. \(P(\emptyset) = 0\)</dd>
      <dt>intersection</dt>
      <dd>\(A \cap B = \set{x : x \in A \land x \in B}\)</dd>
      <dt><i>mutually exclusivei / disjoint</i></dt>
      <dd>if \(A \cap B = \emptyset\)</dd>
      <dt>union</dt>
      <dd>\(A \cup B = \set{x : x \in A \lor x \in B}\)</dd>
      <dt>complement</dt>
      <dd>\(A^\complement = A' = \=A = \set{x : x \not\in A}\)</dd>
    </dl>
    <p><img src="setoperations.png" /></p>
    <dl>
      <dt>rule of addition</dt>
      <dd>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</dd>
      <dt>rule of complement</dt>
      <dd>\(P(A^\complement) = 1 - P(A)\)</dd>
      <dt>demorgan's laws</dt>
      <dd>\((A \cup B)^\complement = A^\complement \cap B^\complement\)</dd>
      <dd>\((A \cap B)^\complement = A^\complement \cup B^\complement\)</dd>
      <dt>distributive laws</dt>
      <dd>\(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\)</dd>
      <dd>\(A \cup (B \cap C) = (A \cup B) \cap (A \cup C)\)</dd>
      <dt>odds <i>in favor of A</i></dt>
      <dd>\(\frac{P(A)}{P(A^\complement)}\)</dd>
      <dd>result between 0 and \(\infty\) exclusive</dd>
    </dl>
    <h2>2.4 counting sample points</h2>
    <dl>
      <dt>combinatorics</dt>
      <dd>study of enumeration</dd>
      <dt>multiplication principle</dt>
      <dd>
        for \(k\) choices, if there are \(n_i\) ways to make choice \(i\), then the
        total number of possibilities is \(\prod\limits_{i = 1}^k n_i\)
      </dd>
      <dt>permutation</dt>
      <dd>an ordered subset</dd>
      <dt>factorial</dt>
      <dd>for any integer \(n \ge 0\), \(n! = \prod\limits_{i = 1}^n i\)</dd>
      <dd>specialcase \(0! = 1\)</dd>
    </dl>
    <p>
      the number of permutations of \(r\) objects among a set of \(n\) is
      \(P_{n, r} = \frac{n!}{(n - r)!}\)
    </p>
    <dl>
      <dt>combination</dt>
      <dd>an unordered subset</dd>
    </dl>
    <p>
      the number of ways to choose \(r\) of \(n\) objects, regardless of order, is \[
        C_{n, r} = {n \choose r} = \frac{P_{n, r}}{r!} = \frac{n!}{r! (n - r)!}
      \]
    </p>
    <p>
      \(n \choose r\) is also called the <b>binomial coefficient</b> and is equal to
      \({n \choose n - r}\)
    </p>
    <dl>
      <dt>multinomial combination</dt>
      <dd>
        if there are \(k\) kinds, then the ways to partition the set into kinds
        is \[{n \choose n_1, n_2, \dots, n_k} = \frac{n!}{\prod_{i = 1}^k n_i!}\]
      </dd>
    </dl>
    <p>
      eg there are \({11 \choose 4, 4, 2, 1} = 34650\) ways to rearrange the letters in
      "mississippi"
    </p>
    <h2>2.4 conditional probability</h2>
    <p>the probability that \(A\) happens <b>given</b> \(B\) is \(
      P(A \mathbin| B) = \frac{P(A \cap B)}{P(B)}
    \)</p>
    <p>this changes the sample space</p>
    <dl>
      <dt>rule of multiplication</dt>
      <dd>\(P(A \cap B) = P(A \mathbin| B)P(B) = P(B \mathbin| A)P(A)\)</dd>
    </dl>
    <p>
      eg if you're rolling a fair die, \(P(\set{5} \mathbin| \set{1, 3, 5}) = \frac13\)
    </p>
    <p>
      two events \(A\) and \(B\) are <b>independent</b> if \(P(A \mathbin| B) = P(A)\),
      or equivalently if \(P(A \cap B) = P(A)(B)\)
    </p>
    <p>two mutually exclusive events are always dependent</p>
    <img src="given-even.png" />
    <img src="given-leq7.png" />
    <h2>2.6 contingency table</h2>
    <dl>
      <dt>contingency table</dt>
      <dd>a table containing all contingent events or categories</dd>
    </dl>
    <table>
      <tr>
        <td class="noborder"></td>
        <th>smoking</th>
        <th>non-<br />smoking</th>
        <th>marginal<br />total</th>
      </tr>
      <tr>
        <th scope="row">bronchitis</th>
        <td>50</td>
        <td>10</td>
        <td><b>60</b></td>
      </tr>
      <tr>
        <th scope="row">no bronchitis</th>
        <td>20</td>
        <td>20</td>
        <td><b>40</b></td>
      </tr>
      <tr>
        <th scope="row">marginal total</th>
        <td><b>70</b></td>
        <td><b>30</b></td>
        <td><b>100</b></td>
      </tr>
    </table>
    <p>eg \(
      P\left(\text{smoking} \middle| \text{bronchitis}^\complement\right) = \frac13
    \)</p>
    <p>smoking and bronchitis are dependent because \(P(S) \neq P(S \mathbin| B)\)</p>
    <h2>2.7 bayes' theorem</h2>
    <p>the events \(B_1, B_2, \dots, B_k\) <b>partition</b> a sample space \(S\) if</p>
    <ul>
      <li>\(B_i \cap B_j = \emptyset\) for all \(i \ne j\)</li>
      <li>\(\bigcup\limits_{1 \le i \le k} B_i = S\)</li>
    </ul>
    <p>
      suppose \(k = 2\) and some event \(A\) has occurred in a random manner in \(S\),
      and we want to know \(P(B_i | A)\). since \(A = (A \cap B_1) \cup (A \cap B_2)\)
      from the partition, \[
        P(B_i | A) = \frac{P(B_i \cap A)}{P(A)} = \frac{P(A | B_i)P(B_i)}{P(A)}
        = \frac{P(A | B_i)P(B_i)}{\sum_j P(A | B_j)P(B_j)}
      \]
    </p>
    <p>
      \(P(B_i | A)\) is the <b>posterior</b> probability
      and \(P(B_i)\) is the <b>prior</b> probability
    </p>
    <h1>ch3 random variables and their probability distributions</h1>
    <dl>
      <dt>random variable</dt>
      <dd>
        a function \(X : \mathbb R \to \mathbb R\) that assigns numerical values to events
        in a sample space
      </dd>
    </dl>
    <p>
      the sample space \(S\) can be either <b>discrete</b> (there are countably many outcomes) or
      <b>continuous</b> (there are uncountable many outcomes)
    </p>
    <p>
      we also define a <b>probability (mass) function</b> \(p\) which takes the numerical
      values of the random variable. we write \(P(X = x)\) or \(p(x)\)
    </p>
    <p>
      example: tossing a coin twice, \(X\) is the number of heads we get (0 or 1 or 2). then we
      can make the <b>probability density function (PDF)</b> of \(X\):
    </p>
    <table>
      <tr>
        <th scope="row" >X</th>
        <td>0</td>
        <td>1</td>
        <td>2</td>
      </tr>
      <tr>
        <th scope="row" >P(X = <i>x</i>)</th>
        <td>\(\frac14\)</td>
        <td>\(\frac12\)</td>
        <td>\(\frac34\)</td>
      </tr>
    </table>
    <p>which has these properties. they look suspiciously similar to the axioms of probability!</p>
    <ul>
      <li>\(0 \le p(x) &lt; 1\) for all values of \(x\)</li>
      <li>\(\sum\limits_{\text{all }x} = 1\)</li>
    </ul>
    <p>if it's continuous,</p>
    <ul>
      <li>\(f(x) \ge 0\) for all \(x\)</li>
      <li>\(\int_{-\infty}^\infty f(x) \;\mathrm dx = 1\)</li>
      <li>\(P(a \le X \le b) = \int_a^b f(x) \;\mathrm dx\)</li>
    </ul>
    <img src="pdf.png" />
    <p>
      the <b>cumulative distribution function (CDF)</b> of a random variable, \(F(x)\),
      is defined as \[
      F_X(x) = P(X \le x) = \sum\limits_{X \le x} p(x)
      \]
    </p>
    <p>for the coin example, \(F(1) = P(X \le 1) = \frac34\) etc</p>
    <p>
      if it's continuous, \[
      F_X(x) = P(X \le x) = \int_{-\infty}^\infty f(t) \;\mathrm dx
      \]
    </p>
    <ul>
      <li>\(\frac{\mathrm d}{\mathrm dx} F(x) = f(x)\)</li>
      <li>P(a \le X \le b) = F(b) - F(a)</li>
      <li>\(F_X(b) = P(X \le b) = \int_{-\infty}^b f(x) \;\mathrm dx\)</li>
    </ul>
    <p>the cdf \(F(x)\) is more informative than the pdf \(f(x)\) about \(X\)</p>
    <p>cdf is nondecreasing</p>
    <p>for continuous variables we can say \(P(X \le a) = P(X &lt; a)\)</p>
    <p>
      <b class="example" >example.</b> let \[
      f(x) = \begin{cases}
      cx^2 - x, & 0 \le x\le 2\\
      0, & \text{otherwise}
      \end{cases}
      \]
      <b class="example" >(a)</b> find \(c\).
    </p>
    <p>
      \[
      \int_0^2 \left(cx^2 + x\right) \;\mathrm dx = 1
      \Leftrightarrow
      \left.\left(\frac c3 x^2 + \frac12 x^2\right)\right|_0^2
      = \frac83 c + 2 = 1
      \]
    </p>
    <p>therefore \(c = -\frac38\)</p>
    <p><b class="example" >(b)</b> find \(F(x)\).</p>
    <p>
      \[
      \int_0^x \left(-\frac38 t^2 + t\right) \;\mathrm dt
      = \left.\left(-\frac18 t^3 + \frac12 t^2\right)\right|_0^x
      = -\frac18 x^3 + \frac12 x^2
      \]
    </p>
    <p>
      therefore,
      \[
      F(x) = \begin{cases}
      0, & x &lt; 0\\
      -\frac18 x^3 + \frac12 x^2, & 0 \le x &lt; 2\\
      1, & x \ge 2
      \end{cases}
      \]
    </p>
    <p><b class="example" >(c.i)</b> evaluate \(P(0 \le X \le 0.5)\) using pdf \(f(x)\)</p>
    <p>
      \[
      \int_0^{0.5} \left(-\frac38 x^2 + x\right) \;\mathrm dx
      = \left.\left(-\frac18 x^3 + \frac12 x^2\right)\right|_0^{0.5}
      = \frac7{64}
      \]
    </p>
    <p>
      (number theory time! \(77 \cdot 73 = (7 \cdot (7 + 1)) \mathbin⧺ (7 \cdot 3) = 5621\) etc)
    </p>
    <p><b class="example" >(c.ii)</b> using cdf \(F(x)\)</p>
    <p>
      \[
      F(0.5) - F(0)
      = -\frac18 \left(\frac12\right)^3 + \frac12 \left(\frac12\right)^2
      = -\frac1{64} + \frac18
      = \frac7{64}
      \]
    </p>
    <script src="../temml.min.js"></script>
    <script>temml.renderMathInElement(document.body, {trust: true});</script>
  </body>
</html>
